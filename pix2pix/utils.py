import os.path
import random

import torch
from loguru import logger
from torchvision.utils import save_image


def save_some_examples(gen: torch.nn.Module, val_loader: torch.utils.data.DataLoader,
                       epoch: int, folder: str, device: torch.device):
    """
    Save some example images generated by the generator.

    Args:
        gen (torch.nn.Module): The generator model.
        val_loader (torch.utils.data.DataLoader): The data loader for validation dataset.
        epoch (int): The current epoch number.
        folder (str): The folder to save the example images.
        device (torch.device): The device to run the generator on.
    """
    x, y = next(iter(val_loader))
    # random_index = random.randint(0, len(val_loader.dataset) - 1)
    # x,y = val_loader.dataset[random_index]
    x, y = x.to(device), y.to(device)
    gen.eval()
    with torch.no_grad():
        y_fake = gen(x)
        y_fake = y_fake * 0.5 + 0.5  # remove normalization#
        save_image(y_fake, os.path.join(folder, f'y_gen_{epoch}.png'))
        save_image(x, os.path.join(folder, f'input_{epoch}.png'))
        save_image(y, os.path.join(folder, f'label_{epoch}.png'))
    gen.train()


def save_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer,
                    filename="my_checkpoint.pth") -> None:
    """
    Save the model and optimizer checkpoints to a file.

    Args:
        model (torch.nn.Module): The model to save.
        optimizer (torch.optim.Optimizer): The optimizer to save.
        filename (str, optional): The filename to save the checkpoint (default: "my_checkpoint.pth").
    """
    logger.info("=> Saving checkpoint")
    checkpoint = {
        "state_dict": model.state_dict(),
        "optimizer": optimizer.state_dict(),
    }
    torch.save(checkpoint, filename)


def load_checkpoint(checkpoint_file: str, model: torch.nn.Module,
                    optimizer: torch.optim.Optimizer, lr: float, device: torch.device):
    """
    Load the model and optimizer checkpoints from a file.
    Args:
        checkpoint_file (str): The path to the checkpoint file.
        model (torch.nn.Module): The model to load the state_dict into.
        optimizer (torch.optim.Optimizer): The optimizer to load the state_dict into.
        lr (float): The learning rate to set for the optimizer.
        device (torch.device): The device to load the checkpoint onto.
   """
    logger.info('=> Loading checkpoints')
    checkpoint = torch.load(checkpoint_file, map_location=device)
    model.load_state_dict(checkpoint["state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer"])

    # If we don't do this then it will just have learning rate of old checkpoint
    # and it will lead to many hours of debugging \:
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr
